{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# health_stopwords = [\"astrazeneca\", \"help\", \"we'r\", \"we're\", \"pfizer\", \"co\", \"http\", \"#jnj\", \"rt\",\"jnj\", \"biogen\", \"gsk\", \"amp\", \"patient\", \"learn\", \"work\",\"today\",\"year\",\"disease\", \"diseases\", \"disorder\", \"symptom\", \"symptoms\", \"drug\", \"drugs\", \"problems\", \"problem\",\"prob\", \"probs\", \"med\", \"meds\", \"pill\", \"pills\", \"medicine\", \"medicines\", \"medication\", \"medications\", \"treatment\", \"treatments\", \"caps\", \"capsules\", \"capsule\", \"tablet\", \"tablets\", \"tabs\", \"doctor\", \"dr\", \"dr.\", \"doc\", \"physician\", \"physicians\", \"test\", \"tests\", \"testing\", \"specialist\", \"specialists\", \"side-effect\", \"side-effects\", \"pharmaceutical\", \"pharmaceuticals\", \"pharma\", \"diagnosis\", 'diagnose', \"diagnosed\", \"exam\",\n",
    "# \"challenge\", \"device\", \"condition\", \"conditions\", \"suffer\", \"suffering\" ,\"suffered\", \"feel\", \"feeling\", \"prescription\", \"prescribe\", \"prescribed\", \"over-the-counter\", \"etc\", \"contact\", \"thank\", \"best\", \"local\", \"women\", \"womeninstem\", \"stem\", \"hi\", 'x', 'y', 'your', 'yours', 'yourself', 'yourselves', 'you', 'yond', 'yonder', 'yon', 'ye', 'yet', 'z', 'zillion', '#####', 'well', 'will', 'who', 'whom', 'underlying', 'condition', 'medical', 'pt', 'patient', 'cont', 'likely', 'including', 'follow', 'required', 'doesnt', 'goes', 'but', 'useful', 'want', 'wants', 'that', 'thats', 'require', 'needed', 'received', 'come', 'asking', 'giving', 'give', 'total', 'developing', 'still', 'presumed', 'believe', 'later', 'just', 'earlier', 'took', 'details', 'liked', 'noticed', 'fairly','non', 'didnt', 'work', 'wasnt', 'www', 'com', 'consistent', 'care', 'called', 'may', 'possible', 'suggest', 'clinical', 'new', 'old', 'family', 'daughter', 'son', 'father', 'mother', 'husband', 'unlikely', 'excluded','group', 'ok', 'being', 'j', 'u', 'umpteen', 'usually', 'us', 'username', 'uponed', 'upons', 'uponing', 'upon', 'ups', 'upping', 'upped', 'up', 'unto', 'until', 'unless', 'unlike', 'unliker', 'unlikest', 'under', 'underneath', 'use','used', 'usedest', 'r', 'rath', 'rather', 'rathest', 'rathe', 'only', 'requested', 're', 'relate', 'related', 'relatively', 'regarding', 'really', 'res', 'respecting', 'respectively', 'q', 'quite', 'que', 'qua', 'n', 'neither', 'neaths', 'neath', 'nethe', 'nethermost', 'necessary', 'necessariest', 'necessarier', 'never', 'nevertheless', 'nigh', 'nighest', 'nigher', 'nine', 'noone', 'nobody', 'nobodies', 'nowhere', 'nowheres', 'no', 'non', 'noes', 'nor', 'nos', 'no-one', 'none', 'not', 'notwithstanding', 'nothings', 'nothing', 'nathless', 'natheless', 't', 'ten', 'tills', 'till', 'tilled', 'tilling', 'to', 'towards', 'toward', 'towardest', 'towarder', 'together', 'too', 'thy', 'thyself', 'thus', 'than', 'that', 'those', 'thou','though', 'thous', 'thouses', 'thoroughest', 'thorougher', 'thorough', 'thoroughly', 'thru', 'thruer', 'thruest', 'thro', 'through', 'throughout', 'throughest', 'througher', 'thine', 'this', 'thises', 'they', 'thee', 'the', 'then', 'thence', 'thenest', 'thener', 'them', 'themselves', 'these', 'therer', 'there', 'thereby', 'therest', 'thereafter', 'therein', 'thereupon', 'therefore', 'their', 'theirs', 'thing', 'things', 'three', 'two', 'o', 'oh', 'owt', 'owning', 'owned', 'own', 'owns', 'others', 'other', 'otherwise', 'otherwisest', 'otherwiser', 'of', 'often', 'oftener', 'oftenest', 'off', 'offs', 'offest', 'one', 'ought', 'oughts', 'our', 'ours', 'ourselves', 'ourself', 'out', 'outest', 'outed', 'outwith', 'outs', 'outside', 'over', 'overallest', 'overaller', 'overalls', 'overall', 'overs', 'or', 'orer', 'orest', 'on', 'oneself', 'onest', 'ons', 'onto','a', 'atween', 'at', 'athwart', 'atop','afore', 'afterward', 'afterwards', 'after', 'afterest', 'afterer', 'ain', 'an', 'any', 'anything', 'anybody', 'anyone', 'anyhow', 'anywhere', 'anent', 'anear', 'and', 'andor', 'another', 'around', 'ares', 'are', 'aest', 'aer', 'against', 'again', 'accordingly', 'abaft', 'abafter', 'abaftest', 'abovest', 'above', 'abover', 'abouter', 'aboutest', 'about', 'aid', 'amidst','amid', 'among', 'amongst', 'apartest', 'aparter', 'apart', 'appeared', 'appears', 'appear', 'appearing', 'appropriating', 'appropriate', 'appropriatest', 'appropriates', 'appropriater', 'appropriated', 'already', 'always', 'also', 'along', 'alongside', 'although', 'almost', 'all', 'allest', 'aller', 'allyou', 'alls', 'albeit', 'awfully', 'as', 'aside', 'asides', 'aslant', 'ases','astrider', 'astride', 'astridest', 'astraddlest', 'astraddler', 'astraddle', 'availablest', 'availabler', 'available', 'aughts', 'aught', 'vs', 'v', 'variousest', 'variouser', 'various', 'via', 'vis-a-vis', 'vis-a-viser', 'vis-a-visest', 'viz', 'very', 'veriest', 'verier', 'versus', 'k', 'g','go', 'gone', 'good', 'got', 'gotta', 'gotten', 'get', 'gets', 'getting', 'b', 'by', 'byandby', 'by-and-by', 'bist', 'both', 'but','buts', 'be', 'beyond', 'because', 'became', 'becomes', 'become', 'becoming', 'becomings', 'becominger', 'becomingest', 'behind', 'behinds', 'before', 'beforehand', 'beforehandest', 'beforehander', 'bettered', 'betters', 'better', 'bettering', 'betwixt', 'between', 'beneath', 'been', 'below', 'besides', 'beside', 'm', 'my', 'myself', 'mucher', 'muchest', 'much', 'must', 'musts', 'musths', 'musth', 'main', 'make', 'mayest', 'many', 'mauger', 'maugre', 'me', 'meanwhiles', 'meanwhile', 'mostly', 'most', 'moreover', 'more', 'might', 'mights', 'midst', 'midsts', 'h', 'huh', 'humph', 'he', 'hers', 'herself', 'her', 'hereby', 'herein', 'hereafters', 'hereafter', 'hereupon', 'hence', 'hadst', 'had', 'having', 'haves', 'have', 'has', 'hast', 'hardly', 'hae', 'hath', 'him', 'himself', 'hither', 'hitherest', 'hitherer', 'his', 'how-do-you-do', 'however', 'how', 'howbeit', 'howdoyoudo', 'hoos', 'hoo', 'w', 'woulded', 'woulding', 'would', 'woulds', 'was', 'wast', 'we', 'wert', 'were', 'with', 'withal', 'without', 'within', 'why', 'what', 'whatever', 'whateverer', 'whateverest', 'whatsoeverer', 'whatsoeverest', 'whatsoever', 'whence', 'whencesoever', 'whenever', 'whensoever', 'when', 'whenas', 'whether', 'wheen', 'whereto', 'whereupon', 'wherever', 'whereon', 'whereof', 'where', 'whereby', 'wherewithal', 'wherewith', 'whereinto', 'wherein', 'whereafter', 'whereas', 'wheresoever', 'wherefrom', 'which', 'whichever', 'whichsoever', 'whilst', 'while', 'whiles', 'whithersoever', 'whither', 'whoever', 'whosoever', 'whoso', 'whose', 'whomever', 's', 'syne', 'syn', 'shalling', 'shall', 'shalled', 'shalls', 'shoulding', 'should', 'shoulded', 'shoulds', 'she', 'sayyid', 'sayid', 'said', 'saider', 'saidest', 'same', 'samest', 'sames', 'samer', 'saved', 'sans', 'sanses', 'sanserifs', 'sanserif', 'so', 'soer', 'soest', 'sobeit', 'someone', 'somebody', 'somehow', 'some', 'somewhere', 'somewhat', 'something', 'sometimest', 'sometimes', 'sometimer', 'sometime', 'several', 'severaler', 'severalest', 'serious', 'seriousest', 'seriouser', 'senza', 'send', 'sent', 'seem', 'seems', 'seemed', 'seemingest', 'seeminger', 'seemings', 'seven', 'summat', 'sups', 'sup', 'supping', 'supped', 'such', 'since', 'sine', 'sines', 'sith','six', 'stop', 'stopped', 'p', 'plaintiff', 'plenty', 'plenties', 'please', 'pleased', 'pleases', 'per', 'perhaps', 'particulars', 'particularly', 'particular', 'particularest', 'particularer', 'pro', 'providing', 'provides', 'provided', 'provide', 'probably', 'l', 'layabout', 'layabouts', 'latter', 'latterest', 'latterer', 'latterly', 'latters', 'lots', 'lotting', 'lotted', 'lot', 'lest', 'less', 'like', 'ie', 'ifs', 'if', 'i', 'info', 'information', 'itself', 'its', 'it', 'is', 'idem', 'idemer', 'idemest', 'immediate', 'immediately', 'immediatest', 'immediater', 'in', 'inwards', 'inwardest', 'inwarder', 'inward', 'inasmuch', 'into', 'instead', 'insofar', 'indicates', 'indicated', 'indicate', 'indicating', 'indeed', 'inc', 'f', 'fact', 'facts', 'fs', 'figupon', 'figupons', 'figuponing', 'figuponed', 'few', 'fewer', 'fewest', 'frae', 'from', 'failing', 'failings', 'five', 'furthers', 'furtherer', 'furthered', 'furtherest', 'further', 'furthering', 'furthermore', 'fourscore', 'followthrough', 'for', 'forwhy', 'fornenst', 'formerly', 'former', 'formerer', 'formerest', 'formers', 'forbye', 'forby', 'fore', 'forever', 'forer', 'fores', 'four', 'd', 'ddays', 'dday', 'do', 'doing', 'doings', 'doe', 'does', 'doth', 'downwarder', 'downwardest', 'downward', 'downwards', 'downs', 'done', 'doner', 'dones', 'donest', 'dos', 'dost', 'did', 'differentest', 'differenter', 'different', 'describing', 'describe', 'describes', 'described', 'despiting', 'despites', 'despited', 'despite', 'during', 'c', 'cum', 'circa', 'chez', 'cer', 'certain', 'certainest', 'certainer', 'cest', 'canst', 'cannot', 'cant', 'cants', 'canting', 'cantest', 'canted', 'co', 'could', 'couldst', 'comeon', 'comeons', 'come-ons', 'come-on', 'concerning', 'concerninger', 'concerningest', 'consequently', 'considering',  'e', 'eg', 'eight', 'either', 'even', 'evens', 'evenser', 'evensest', 'evened', 'evenest', 'ever', 'everyone', 'everything', 'everybody', 'everywhere', 'every', 'ere', 'each', 'et', 'etc', 'elsewhere', 'else', 'ex', 'excepted', 'excepts', 'except', 'excepting', 'exes', 'enough', '&quot', '&quot;', '&amp', '&amp;(.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    # Lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove all non-alphabets (punctuation, numbers, new-line characters and extra-spaces)\n",
    "    sentence = re.sub(r'[^a-zA-Z]+', ' ', sentence)\n",
    "    sentence = sentence.replace('\\n', '')\n",
    "    # Remove URLs\n",
    "    sentence = re.sub(r'https\\S+', '', sentence)\n",
    "    # Remove double spacing\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    \n",
    "    # Tokenize & remove stop-words\n",
    "    word_list = nltk.word_tokenize(sentence)    \n",
    "    stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "    stopwords_list.extend(['thank','today','join','astrazeneca','pfizer','biogen','johnson','amp','gsk','jnj','lt'])\n",
    "    word_list = [word for word in word_list if word not in stopwords_list]\n",
    "    \n",
    "    # Remove very small words, length < 3, they don't contribute any useful information\n",
    "    word_list = [word for word in word_list if len(word) > 3]\n",
    "        \n",
    "    # Stem & Lemmatize\n",
    "    porter_stemmer = nltk.stem.PorterStemmer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    word_list = [porter_stemmer.stem(word) for word in word_list]\n",
    "    word_list = [lemmatizer.lemmatize(word) for word in word_list]\n",
    "    \n",
    "    sentence = ' '.join(word_list)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PHARMA_PATH = '../../data/twitter/pharma companies'\n",
    "GOVT_INSTITUTES_PATH = '../../data/twitter/govt institutes'\n",
    "NGO_PATH = '../../data/twitter/ngo'\n",
    "\n",
    "EPOCHS = 205\n",
    "TOPICS = 5\n",
    "CHUNK_SIZE = 1000\n",
    "WORKERS = 7\n",
    "EVAL_PERIOD = 10\n",
    "ALPHA = 0.01\n",
    "BETA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pharma_df = pd.concat([pd.read_csv(f, sep=',') for f in glob.glob(PHARMA_PATH + \"/*.csv\")],ignore_index=True)\n",
    "govt_institutes_df = pd.concat([pd.read_csv(f, sep=',') for f in glob.glob(GOVT_INSTITUTES_PATH + \"/*.csv\")],ignore_index=True)\n",
    "ngo_df = pd.concat([pd.read_csv(f, sep=',') for f in glob.glob(NGO_PATH + \"/*.csv\")],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pharma_df.shape, govt_institutes_df.shape, ngo_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pharma_df,govt_institutes_df,ngo_df],ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide dataset as per timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'created_at' column to datetime\n",
    "df['created_at'] = df['created_at'].str[:-6]\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Sort by datetime ascending\n",
    "df = df.sort_values(by='created_at')\n",
    "\n",
    "# Pre-process the dataset\n",
    "tqdm.tqdm.pandas()\n",
    "df['tweet_tokenized'] = df['tweet'].progress_apply(lambda x:preprocess_text(str(x)))\n",
    "\n",
    "# Divide as per dates\n",
    "pre_covid_df = df.loc[df['created_at'] <= '2020-02-26 23:59:59']\n",
    "print(pre_covid_df.shape)\n",
    "\n",
    "post_covid_df = df.loc[df['created_at'] >= '2020-02-27 00:00:00']\n",
    "print(post_covid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-COVID "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TF-IDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pre_covid_df['tweet_tokenized'].str.split()\n",
    "dictionary = gensim.corpora.Dictionary(documents)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=20000)\n",
    "\n",
    "tfidf_model = gensim.models.TfidfModel(dictionary=dictionary)\n",
    "\n",
    "corpus = [dictionary.doc2bow(d) for d in documents]\n",
    "corpus_tfidf = list(tfidf_model[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics = pd.DataFrame(columns=['feature-extraction','clustering-algo','run#', 'state', 'c_v', 'c_umass','topics','time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(2, 6, 1), range(4, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_lda = gensim.models.ldamodel.LdaModel(corpus=corpus_tfidf, num_topics=TOPICS, id2word=dictionary, chunksize=CHUNK_SIZE, passes=EPOCHS, \n",
    "                                                     eval_every = EVAL_PERIOD, per_word_topics=True, random_state=state, alpha=ALPHA, eta=BETA)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_lda, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_lda, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_lda.print_topics()\n",
    "    \n",
    "    stop = time.time()\n",
    "    \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'LDA', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel LDA (LDA Multicore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(1, 6, 1), range(2, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_plda = gensim.models.ldamulticore.LdaMulticore(corpus=corpus_tfidf, num_topics=TOPICS, id2word=dictionary, chunksize=CHUNK_SIZE, workers=WORKERS, passes=EPOCHS, \n",
    "                                                     eval_every = EVAL_PERIOD, per_word_topics=True, random_state=state, alpha=ALPHA, eta=BETA)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_plda, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_plda, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_plda.print_topics()\n",
    "    \n",
    "    stop = time.time()\n",
    "    \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'Parallel LDA', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(1, 6, 1), range(2, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_nmf = gensim.models.Nmf(corpus=corpus_tfidf, num_topics=TOPICS, id2word=dictionary, chunksize=CHUNK_SIZE, passes=EPOCHS, eval_every=EVAL_PERIOD, minimum_probability=0, \n",
    "                                   random_state=state, kappa=1)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_nmf, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_nmf, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_nmf.print_topics()\n",
    "    \n",
    "    stop = time.time()\n",
    "    \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'NMF', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(1, 6, 1), range(2, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_lsi = gensim.models.LsiModel(corpus=corpus_tfidf, num_topics=TOPICS, id2word=dictionary, chunksize=CHUNK_SIZE)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_lsi, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_lsi, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_lsi.print_topics()\n",
    "    stop = time.time()\n",
    "      \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'LSI', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(1, 6, 1), range(2, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_hdp = gensim.models.hdpmodel.HdpModel(corpus=corpus_tfidf, id2word=dictionary, chunksize=CHUNK_SIZE, random_state=state, kappa=1, alpha=ALPHA)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_hdp, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_hdp, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_hdp.print_topics()\n",
    "    stop = time.time()\n",
    "      \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'HDP', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics.to_csv('../../results/content-analysis/topic-modelling/pre-covid-nmf.csv',index=False)\n",
    "\n",
    "mean_perf = performance_metrics.groupby('clustering-algo')[['c_v','c_umass','time']].mean()\n",
    "mean_perf.to_csv('../../results/content-analysis/topic-modelling/pre-covid-mean-nmf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = post_covid_df['tweet_tokenized'].str.split()\n",
    "dictionary = gensim.corpora.Dictionary(documents)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=20000)\n",
    "\n",
    "tfidf_model = gensim.models.TfidfModel(dictionary=dictionary)\n",
    "\n",
    "corpus = [dictionary.doc2bow(d) for d in documents]\n",
    "corpus_tfidf = list(tfidf_model[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_covid_performance_metrics = pd.DataFrame(columns=['feature-extraction','clustering-algo','run#', 'state', 'c_v', 'c_umass', 'topics','time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(2, 6, 1), range(4, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_lda = gensim.models.ldamodel.LdaModel(corpus=corpus_tfidf, num_topics=TOPICS, id2word=dictionary, chunksize=CHUNK_SIZE, passes=EPOCHS, \n",
    "                                                     eval_every = EVAL_PERIOD, per_word_topics=True, random_state=state, alpha=ALPHA, eta=BETA)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_lda, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_lda, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_lda.print_topics()\n",
    "    \n",
    "    stop = time.time()\n",
    "    \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'LDA', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel LDA (LDA Multicore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(1, 6, 1), range(2, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_plda = gensim.models.ldamulticore.LdaMulticore(corpus=corpus_tfidf, num_topics=TOPICS, id2word=dictionary, chunksize=CHUNK_SIZE, workers=WORKERS, passes=EPOCHS, \n",
    "                                                     eval_every = EVAL_PERIOD, per_word_topics=True, random_state=state, alpha=ALPHA, eta=BETA)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_plda, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_plda, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_plda.print_topics()\n",
    "    \n",
    "    stop = time.time()\n",
    "    \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'Parallel LDA', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(1, 6, 1), range(2, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_nmf = gensim.models.Nmf(corpus=corpus_tfidf, num_topics=TOPICS, id2word=dictionary, chunksize=CHUNK_SIZE, passes=EPOCHS, eval_every=EVAL_PERIOD, minimum_probability=0, \n",
    "                                   random_state=state, kappa=1)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_nmf, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_nmf, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_nmf.print_topics()\n",
    "    \n",
    "    stop = time.time()\n",
    "    \n",
    "    post_covid_performance_metrics = post_covid_performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'NMF', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(1, 6, 1), range(2, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_lsi = gensim.models.LsiModel(corpus=corpus_tfidf, num_topics=TOPICS, id2word=dictionary, chunksize=CHUNK_SIZE)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_lsi, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_lsi, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_lsi.print_topics()\n",
    "    stop = time.time()\n",
    "      \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'LSI', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, state in zip(range(1, 6, 1), range(2, 12, 2)):\n",
    "    print('Run #', run)\n",
    "    \n",
    "    start = time.time()\n",
    "    gensim_hdp = gensim.models.hdpmodel.HdpModel(corpus=corpus_tfidf, id2word=dictionary, chunksize=CHUNK_SIZE, random_state=state, kappa=1, alpha=ALPHA)\n",
    "    \n",
    "    coherence_cv = gensim.models.CoherenceModel(model=gensim_hdp, texts=documents, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    coherence_cumass = gensim.models.CoherenceModel(model=gensim_hdp, texts=documents, dictionary=dictionary, coherence='u_mass').get_coherence()\n",
    "    topics = gensim_hdp.print_topics()\n",
    "    stop = time.time()\n",
    "      \n",
    "    performance_metrics = performance_metrics.append({'feature-extraction':'tf-idf', 'clustering-algo':'HDP', 'run#':run, 'state':state,'c_v':coherence_cv,'c_umass':coherence_cumass,\n",
    "                                                      'topics':topics,'time':(stop-start)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_covid_performance_metrics.to_csv('../../results/content-analysis/topic-modelling/post-covid-nmf.csv',index=False)\n",
    "\n",
    "mean_perf = post_covid_performance_metrics.groupby('clustering-algo')[['c_v','c_umass','time']].mean()\n",
    "mean_perf.to_csv('../../results/content-analysis/topic-modelling/post-covid-mean-nmf.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
